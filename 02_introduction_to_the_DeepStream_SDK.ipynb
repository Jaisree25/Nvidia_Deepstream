{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb21aed8",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1852a",
   "metadata": {},
   "source": [
    "# Introduction to the DeepStream SDK #\n",
    "The DeepStream SDK is a streaming analytics toolkit that can be used to build video AI applications. It simplifies the process by letting developers combine existing or custom plugins to construct video processing pipelines for their specific use case. DeepStream makes it easier than ever to get started building and deploying AI-based intelligent video analytics applications. \n",
    "\n",
    "When developing intelligent video analytics solutions, DeepStream helps users tackle laborious tasks like:\n",
    "* Leverage hardware for accelerated processing\n",
    "* Optimize pipeline for high data-throughput and low latency\n",
    "* Optimize neural network model for high-speed inference\n",
    "* Process data from multiple video streams simultaneously\n",
    "* Keep track of metadata associated with each frame of a video\n",
    "\n",
    "In doing so, we enable developers to prioritize important business decisions like: \n",
    "* Kind and number of video streams to analyze\n",
    "* Type(s) of video analytics to perform\n",
    "* Post-processing of the AI inference results\n",
    "\n",
    "The DeepStream SDK allows developers to focus on the more *important* tasks related to the project's goal and impact. It empowers developers to build core deep learning networks and IP rather than design end-to-end solutions from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8abfead",
   "metadata": {},
   "source": [
    "## Learning Objectives ##\n",
    "In this notebook, you will gain the foundational understanding necessary to use the NVIDIA DeepStream SDK effectively, including: \n",
    "* History of GStreamer and DeepStream\n",
    "* Anatomy of a DeepStream Video AI Pipeline\n",
    "* Different Types of DeepStream Plugins\n",
    "* How Data Flow Through a DeepStream Pipeline\n",
    "\n",
    "**Table of Contents**\n",
    "<br>\n",
    "This notebook covers the below sections: \n",
    "1. [Sample Video AI Application](#s1)\n",
    "    * [Video Formats](#s1.1)\n",
    "    * [Exercise #1 - Run Sample Application](#e1)\n",
    "2. [GStreamer Foundations](#s2)\n",
    "3. [Anatomy of a DeepStream Pipeline](#s3)\n",
    "    * [Inspecting Plugins](#s3.1)\n",
    "    * [Exercise #2 - Explore Plugins](#e2)\n",
    "4. [Access Insights Generated from AI Inference](#s4)\n",
    "    * [Probe](#s4.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa2935c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s1'></a>\n",
    "## Sample Video AI Application ##\n",
    "Let's look at a sample video AI application. In this lab, we will build DeepStream pipelines to analyze a parking garage camera feed. This sample application uses the same pipeline we will construct in the next notebook. For demonstration, we refactored the procedure into a [Python script](sample_apps/app_02.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b2cf34-f0ed-4366-bc8b-d89329648724",
   "metadata": {},
   "source": [
    "<a name='s1.1'></a>\n",
    "### Video Formats ###\n",
    "The input video file is an encoded video file with a **.h264** extension, which is perhaps not the **.mp4** extension we would expect for a video file. The .mp4 file extension is a representation of the container, which has all the files needed to play back a video. These files include the visual images, the audio tracks, and the metadata (i.e., bitrate, resolution, subtitles, timestamp, etc.). The metadata also contains information about the **codec** used for the audio and video streams. The codec, which is a mashup of the words *co*de and *dec*ode, is a method used to compress (encode) a video into a smaller size for faster transmission. The encoded file can be decompressed (decoded) using the same codec for playback and processing. The most common video codecs include **[H.264](https://en.wikipedia.org/wiki/Advanced_Video_Coding)**, **[H.265](https://en.wikipedia.org/wiki/High_Efficiency_Video_Coding)**, and **[MPEG4](https://en.wikipedia.org/wiki/MPEG-4)**. Separate from MPEG4, **[MP4](https://en.wikipedia.org/wiki/MPEG-4_Part_14)** is a container that can be used for playback in the JupyterLab. These properties describe the video format and new ones are continuously being developed to provide improvements in quality, file size, and video playback. We need to build the application based on the video format(s) of the input and desire output. \n",
    "\n",
    "<p><img src='images/important.png' width=720></p>\n",
    "When performing video analytics, it is likely that the application will consume H.264 encoded video streams instead of MP4 container files since only the video component is needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa94b1",
   "metadata": {},
   "source": [
    "<a name='e1'></a>\n",
    "#### Exercise #1 - Run Sample Application ####\n",
    "\n",
    "**Instructions**: <br>\n",
    "* Execute the below cell to convert the H.264 encoded video file, which can't be played in JupyterLab, into a MP4 file for playback. \n",
    "    * The [FFmpeg](https://ffmpeg.org/) tool is a very fast video and audio converter with the general syntax: <br> `ffmpeg [global_options] {[input_file_options] -i input_url} ... {[output_file_options] output_url} ...`. <br> When using the `ffmpeg` command, the `-i` option lets us read an input URL, the `-loglevel quiet` option suppresses the logs, and the `-y` flag overwrites any existing output file with the same name. \n",
    "* Execute the cell below to see the converted input video. \n",
    "* Execute the cell below to run the DeepStream pipeline. Since we designed the pipeline to write an encoded output file using the MPEG4 codec, we also convert it into a MP4 container file for playback. \n",
    "* Execute the cell below after to convert the MPEG4 encoded video output file into a MP4 file and play the output video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf98c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"input_vid.mp4\" controls  width=\"720\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "from IPython.display import Video\n",
    "\n",
    "!ffmpeg -i /mnt/c/Users/drjai/Nvidia_Deepstream/input_vid.mp4 -c:v libx264 -crf 23 /mnt/c/Users/drjai/Nvidia_Deepstream/data/sample_vid.h264 -y -loglevel quiet\n",
    "# Convert the H.264 encoded video file to MP4 container file - this will generate the sample_30.mp4 file\n",
    "#!ffmpeg -i /mnt/c/Users/drjai/AI Video Nvidia/data/sample_30.h264 /mnt/c/Users/drjai/AI Video Nvidia/sample_30.mp4 \\\n",
    "        #-y \\\n",
    "        #-loglevel quiet\n",
    "\n",
    "# View the input video\n",
    "Video('input_vid.mp4', width=720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6345728-212f-4587-acf6-091ae571f785",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "File `'sample_apps/app_2.py'` not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/launcher/lib/python3.10/site-packages/IPython/core/magics/execution.py:716\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    715\u001b[0m     fpath \u001b[38;5;241m=\u001b[39m arg_lst[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 716\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43mfile_finder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/launcher/lib/python3.10/site-packages/IPython/utils/path.py:91\u001b[0m, in \u001b[0;36mget_py_filename\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m py_name\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile `\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m` not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n",
      "\u001b[0;31mOSError\u001b[0m: File `'sample_apps/app_2.py'` not found.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# DO NOT CHANGE THIS CELL\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Run the DeepStream pipeline - this will generate the output_02_encoded.mpeg4 file\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msample_apps/app_2.py data/sample_vid.h264\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/launcher/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/launcher/lib/python3.10/site-packages/IPython/core/magics/execution.py:727\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\"\u001b[39m,fpath):\n\u001b[1;32m    726\u001b[0m         warn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFor Windows, use double quotes to wrap a filename: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124mun \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmypath\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmyfile.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 727\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fpath \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmeta_path:\n",
      "\u001b[0;31mException\u001b[0m: File `'sample_apps/app_2.py'` not found."
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Run the DeepStream pipeline - this will generate the output_02_encoded.mpeg4 file\n",
    "%run sample_apps/app_2.py data/sample_vid.h264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4a9f888-0de7-46e1-97f3-639cd313281e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"output_02.mp4\" controls  width=\"720\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Convert the encoded video file for playback - this will generate the output_02.mp4 file\n",
    "!ffmpeg -i /dli/task/output_02_encoded.mpeg4 /dli/task/output_02.mp4 \\\n",
    "        -y \\\n",
    "        -loglevel quiet\n",
    "\n",
    "# View the output video\n",
    "Video('output_02.mp4', width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f20718",
   "metadata": {},
   "source": [
    "<a name='s2'></a>\n",
    "## GStreamer Foundations ##\n",
    "DeepStream utilizes an optimized graph architecture built using the open-source [GStreamer multimedia framework](https://gstreamer.freedesktop.org/). GStreamer is used for creating streaming media applications, ranging from a simple media player to complex video editing applications. GStreamer plugins can be mixed and matched into arbitrary pipelines to create custom applications. \n",
    "\n",
    "There are a few key concepts in GStreamer that we need to know before building our application. Understanding the terminologies and their roles in the software will help us rationalize the syntax for working with GStreamer and DeepStream. \n",
    "* **Elements** - Elements are at the core of GStreamer. Elements provide some sort of functionality when linked with other elements. For example, a source element provides data to a stream, a filter element processes a stream of data, and a sink element consumes data. Data flow downstream from source elements to sink elements, passing through filter elements. GStreamer offers a large collection of elements by default but also allows for writing new elements. \n",
    "    * A [sink](https://en.wikipedia.org/wiki/Sink_(computing)), in computing, is designed to receive data. \n",
    "* **Bins** - Bins are container elements that allow you to combine linked elements into a logical group. Bins can be handled in the same way as any other element. They are programmed to manage elements contained within, including state changes as well as bus messages, to ensure that data flow smoothly. This is useful when constructing complex pipelines that require many elements. \n",
    "* **Pipeline** - A pipeline is the top-level bin that also manages the synchronization and bus messages of the contained elements. \n",
    "* **Plugins** - Elements need to be encapsulated in a plugin to enable GStreamer to use it. A plugin is essentially a loadable block of code, usually recognized as a shared object file or a dynamically linked library. A plugin may contain the implementation of several elements, or just one. GStreamer provides building blocks in the form of plugins that can be used to construct an efficient video analytics pipeline. The DeepStream SDK features hardware-accelerated plugins that bring deep neural networks and other complex processing tasks into the stream processing pipeline. \n",
    "* **Bus** - The bus is the object responsible for delivering to the application **messages** generated by the elements. Every pipeline contains a bus by default, so the only thing applications should do is set a message handler on a bus, which is like a signal handler to an object. When the main loop is running, the bus will periodically be checked for new messages, and the message handler will be called when any new message is available. \n",
    "    * Messages signal the application of pipeline events. Some of the message types include `GST_MESSAGE_EOS` (end-of-stream), `GST_MESSAGE_ERROR`, and `GST_MESSAGE_WARNING`. \n",
    "* **Pads** - Pads are used to negotiate links and dataflow between elements in GStreamer. A pad is the “port” on an element where links can be made with other elements for data to flow through. When data flow from element to element in a pipeline, in reality it flows from the source pad of one element to the sink pad of another. Links are only allowed between two pads when the data types, or **capabilities**, are compatible. \n",
    "* **Buffers** and **Events** - All streams of data in GStreamer are chopped up into chunks and passed from a source pad of one element to a sink pad of another element as one of the two types of `GstMiniObject`: **events** (control) and **buffers** (content). A buffer is the basic unit of data transfer in GStreamer. Normally, it contains a chunk of video data that flow from one element to another. The DeepStream SDK attaches the DeepStream metadata object, `NvDsBatchMeta`, to the buffer. An event, on the other hand, contains information on the state of the stream flowing between two linked pads. Events can be used to indicate the end of a media stream. \n",
    "* **Queries** - Queries are used to get information about the stream. \n",
    "\n",
    "<p><img src='images/important.png' width=720></p>\n",
    "\n",
    "For the most part, all data in GStreamer flow one way through a link between elements. When data flow from one DeepStream element to another, the buffers are not recreated. Instead, buffer pointers are passed to avoid unnecessary copies and achieve high-speed performance. \n",
    "\n",
    "<p><img src='images/gstreamer.png' width='720px'></p>\n",
    "\n",
    "For more information, please refer to [GStreamer Basics](https://gstreamer.freedesktop.org/documentation/application-development/basics/index.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6190259",
   "metadata": {},
   "source": [
    "<a name='s3'></a>\n",
    "## Anatomy of a DeepStream Pipeline ##\n",
    "GStreamer and by extension DeepStream applications have a **plugin-based architecture**. Developers can interact with elements through the plugins they are encapsulated in. One single **plugin** may contain the implementation of several elements, or just one. It performs a specific function and has been created for the convenience of developers to leverage. When building a pipeline, we can select from a catalogue of available [GStreamer](https://gstreamer.freedesktop.org/documentation/plugins_doc.html) or [DeepStream](https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_Intro.html#) plugins, or create new ones. An application can be thought of as a pipeline consisting of individual components (plugins), each representing a functional block like video decoding/encoding, scaling, inferencing, and more. \n",
    "\n",
    "The graph below shows the pipeline of a typical video analytics application, starting from consuming input videos to outputting insights. All the individual blocks are various plugins that are used. At the bottom are the different hardware engines that are utilized throughout the application. Where applicable, plugins are accelerated using the underlying hardware to deliver maximum performance. This could involve optimum memory management with zero-memory copy between plugins as well as the use of various accelerators to ensure the highest performance. \n",
    "<p><img src='images/deepstream_overview_graph_architecture.png' width='720px'></p>\n",
    "\n",
    "* Streaming data can come over the network through RTSP or from a local file system or from a camera directly. The streams are captured using the CPU. Once the frames are in the memory, they are sent for decoding using the NVDEC accelerator. \n",
    "* After decoding, there is an _optional_ image pre-processing step where the input image can be pre-processed before inference. The pre-processing can be image dewarping or color space conversion. `Gst-nvdewarper` plugin can dewarp the image from a fisheye or 360-degree camera. `Gst-nvvideoconvert` plugin can perform color format conversion on the frame. These plugins use GPU or VIC (vision image compositor).\n",
    "* The next step is to batch the frames for optimal inference performance. Batching is done using the `Gst-nvstreammux` plugin.\n",
    "* Once frames are batched, it is sent for inference. The inference can be done using TensorRT, NVIDIA’s inference accelerator runtime or can be done in the native framework such as TensorFlow or PyTorch using Triton Inference Server. TensorRT inference is performed using `Gst-nvinfer` plugin and inference using Triton is done using `Gst-nvinferserver` plugin. \n",
    "* After inference, the next step could involve tracking the object. There are several built-in reference trackers in the SDK, ranging from high performance to high accuracy. Object tracking is performed using the `Gst-nvtracker` plugin.\n",
    "* For creating visualization artifacts such as bounding boxes, segmentation masks, labels there is a visualization plugin called `Gst-nvdsosd`.\n",
    "* Finally, to output the results, DeepStream presents various options: render the output with the bounding boxes on the screen, save the output to the local disk, stream out over RTSP, or just send the metadata to the cloud. For sending metadata to the cloud, DeepStream uses `Gst-nvmsgconv` and `Gst-nvmsgbroker` plugin. `Gst-nvmsgconv` converts the metadata into schema payload and `Gst-nvmsgbroker` establishes the connection to the cloud and sends the telemetry data. There are several built-in broker protocols such as Kafka, MQTT, AMQP and Azure IoT. Custom broker adapters can be created.\n",
    "\n",
    "By connecting different plugins into a pipeline, we can build complex applications for custom use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a41107-5eb6-4608-9a03-06f8a2f47dc4",
   "metadata": {},
   "source": [
    "<a name='s3.1'></a>\n",
    "### Inspecting Plugins ### \n",
    "We can inspect plugins using `gst-inspect-1.0`. It's a tool that prints out information on available plugins, information about a particular plugin, or information about a particular element. When executed with no *plugin* or *element* argument, it will print a list of all plugins and elements together with a summary. When executed with a *plugin* or *element* argument, it will print information about that plugin or element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c7fda3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mcapsfilter\u001b[0m: \u001b[0mCapsFilter\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mclocksync\u001b[0m: \u001b[0mClockSync\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mconcat\u001b[0m: \u001b[0mConcat\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mdataurisrc\u001b[0m: \u001b[0mdata: URI source element\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mdownloadbuffer\u001b[0m: \u001b[0mDownloadBuffer\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mfakesink\u001b[0m: \u001b[0mFake Sink\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mfakesrc\u001b[0m: \u001b[0mFake Source\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mfdsink\u001b[0m: \u001b[0mFiledescriptor Sink\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mfdsrc\u001b[0m: \u001b[0mFiledescriptor Source\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mfilesink\u001b[0m: \u001b[0mFile Sink\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mfilesrc\u001b[0m: \u001b[0mFile Source\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mfunnel\u001b[0m: \u001b[0mFunnel pipe fitting\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32midentity\u001b[0m: \u001b[0mIdentity\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32minput-selector\u001b[0m: \u001b[0mInput selector\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mmultiqueue\u001b[0m: \u001b[0mMultiQueue\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32moutput-selector\u001b[0m: \u001b[0mOutput selector\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mqueue\u001b[0m: \u001b[0mQueue\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mqueue2\u001b[0m: \u001b[0mQueue 2\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mstreamiddemux\u001b[0m: \u001b[0mStreamid Demux\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mtee\u001b[0m: \u001b[0mTee pipe fitting\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mtypefind\u001b[0m: \u001b[0mTypeFind\u001b[0m\n",
      "\u001b[94mcoreelements\u001b[0m:  \u001b[32mvalve\u001b[0m: \u001b[0mValve element\u001b[0m\n",
      "\u001b[94mcoretracers\u001b[0m:  \u001b[32mfactories\u001b[0m (\u001b[0mGstTracerFactory\u001b[0m)\n",
      "\u001b[94mcoretracers\u001b[0m:  \u001b[32mlatency\u001b[0m (\u001b[0mGstTracerFactory\u001b[0m)\n",
      "\u001b[94mcoretracers\u001b[0m:  \u001b[32mleaks\u001b[0m (\u001b[0mGstTracerFactory\u001b[0m)\n",
      "\u001b[94mcoretracers\u001b[0m:  \u001b[32mlog\u001b[0m (\u001b[0mGstTracerFactory\u001b[0m)\n",
      "\u001b[94mcoretracers\u001b[0m:  \u001b[32mrusage\u001b[0m (\u001b[0mGstTracerFactory\u001b[0m)\n",
      "\u001b[94mcoretracers\u001b[0m:  \u001b[32mstats\u001b[0m (\u001b[0mGstTracerFactory\u001b[0m)\n",
      "\u001b[94mstaticelements\u001b[0m:  \u001b[32mbin\u001b[0m: \u001b[0mGeneric bin\u001b[0m\n",
      "\u001b[94mstaticelements\u001b[0m:  \u001b[32mpipeline\u001b[0m: \u001b[0mPipeline object\u001b[0m\n",
      "\n",
      "\u001b[94mTotal count\u001b[0m: \u001b[0m3 plugins\u001b[0m, \u001b[0m30 features\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "!gst-inspect-1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ba6b3f-3b69-4833-9208-d4f663b9a1bd",
   "metadata": {},
   "source": [
    "There are numerous plugins available for developers to use. You can learn more about them in the documentations for [GStreamer Plugins](https://gstreamer.freedesktop.org/documentation/plugins_doc.html) and [DeepStream Plugins](https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_Intro.html#). Let's now inspect a specific plugin to learn more about it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7952f1ba-5172-4996-9c7c-e6487bf300e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No such element or plugin 'h264parse'\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "!gst-inspect-1.0 h264parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a08700",
   "metadata": {},
   "source": [
    "We get a lot of useful information, but for now we focus on the _description_. By inspecting the `h264parse` plugin, we see that this is intended for parsing H.264 streams. Video data are typically streamed in encoded format to be efficient. We commonly use [H.264](https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC) for compression and encoding, but other options like H.265, VC1, and MPEG-2, to name a few, are available. Compression facilitates accelerated processing by reducing the amount of data transmitted from one place to another. When building a pipeline, we can use this plugin if we need to parse H.264 video streams. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492df9db",
   "metadata": {},
   "source": [
    "<a name='e2'></a>\n",
    "#### Exercise #2 - Explore Plugins ####\n",
    "Let's inspect a DeepStream-specific plugin: `nvinfer`. \n",
    "\n",
    "**Instructions**: <br>\n",
    "* Modify the below cell by changing the `<FIXME>` only prior to executing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b7cd2e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory Details:\n",
      "  Rank                     primary (256)\n",
      "  Long-name                NvInfer plugin\n",
      "  Klass                    NvInfer Plugin\n",
      "  Description              Nvidia DeepStreamSDK TensorRT plugin\n",
      "  Author                   NVIDIA Corporation. Deepstream for Tesla forum: https://devtalk.nvidia.com/default/board/209\n",
      "\n",
      "Plugin Details:\n",
      "  Name                     nvdsgst_infer\n",
      "  Description              NVIDIA DeepStreamSDK TensorRT plugin\n",
      "  Filename                 /usr/lib/x86_64-linux-gnu/gstreamer-1.0/deepstream/libnvdsgst_infer.so\n",
      "  Version                  6.0.0\n",
      "  License                  Proprietary\n",
      "  Source module            nvinfer\n",
      "  Binary package           NVIDIA DeepStreamSDK TensorRT plugin\n",
      "  Origin URL               http://nvidia.com/\n",
      "\n",
      "GObject\n",
      " +----GInitiallyUnowned\n",
      "       +----GstObject\n",
      "             +----GstElement\n",
      "                   +----GstBaseTransform\n",
      "                         +----GstNvInfer\n",
      "\n",
      "Pad Templates:\n",
      "  SRC template: 'src'\n",
      "    Availability: Always\n",
      "    Capabilities:\n",
      "      video/x-raw(memory:NVMM)\n",
      "                 format: { (string)NV12, (string)RGBA }\n",
      "                  width: [ 1, 2147483647 ]\n",
      "                 height: [ 1, 2147483647 ]\n",
      "              framerate: [ 0/1, 2147483647/1 ]\n",
      "  \n",
      "  SINK template: 'sink'\n",
      "    Availability: Always\n",
      "    Capabilities:\n",
      "      video/x-raw(memory:NVMM)\n",
      "                 format: { (string)NV12, (string)RGBA }\n",
      "                  width: [ 1, 2147483647 ]\n",
      "                 height: [ 1, 2147483647 ]\n",
      "              framerate: [ 0/1, 2147483647/1 ]\n",
      "\n",
      "Element has no clocking capabilities.\n",
      "Element has no URI handling capabilities.\n",
      "\n",
      "Pads:\n",
      "  SINK: 'sink'\n",
      "    Pad Template: 'sink'\n",
      "  SRC: 'src'\n",
      "    Pad Template: 'src'\n",
      "\n",
      "Element Properties:\n",
      "  name                : The name of the object\n",
      "                        flags: readable, writable\n",
      "                        String. Default: \"nvinfer0\"\n",
      "  parent              : The parent of the object\n",
      "                        flags: readable, writable\n",
      "                        Object of type \"GstObject\"\n",
      "  qos                 : Handle Quality-of-Service events\n",
      "                        flags: readable, writable\n",
      "                        Boolean. Default: false\n",
      "  unique-id           : Unique ID for the element. Can be used to identify output of the element\n",
      "                        flags: readable, writable, changeable only in NULL or READY state\n",
      "                        Unsigned Integer. Range: 0 - 4294967295 Default: 15 \n",
      "  process-mode        : Infer processing mode\n",
      "                        flags: readable, writable, changeable only in NULL or READY state\n",
      "                        Enum \"GstNvInferProcessModeType\" Default: 1, \"primary\"\n",
      "                           (1): primary          - Primary (Full Frame)\n",
      "                           (2): secondary        - Secondary (Objects)\n",
      "  config-file-path    : Path to the configuration file for this instance of nvinfer\n",
      "                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state\n",
      "                        String. Default: \"\"\n",
      "  infer-on-gie-id     : Infer on metadata generated by GIE with this unique ID.\n",
      "\t\t\tSet to -1 to infer on all metadata.\n",
      "                        flags: readable, writable, changeable only in NULL or READY state\n",
      "                        Integer. Range: -1 - 2147483647 Default: -1 \n",
      "  infer-on-class-ids  : Operate on objects with specified class ids\n",
      "\t\t\tUse string with values of class ids in ClassID (int) to set the property.\n",
      "\t\t\t e.g. 0:2:3\n",
      "                        flags: readable, writable, changeable only in NULL or READY state\n",
      "                        String. Default: \"\"\n",
      "  filter-out-class-ids: Ignore metadata for objects of specified class ids\n",
      "\t\t\tUse string with values of class ids in ClassID (int) to set the property.\n",
      "\t\t\t e.g. 0;2;3\n",
      "                        flags: readable, writable, changeable only in NULL or READY state\n",
      "                        String. Default: \"\"\n",
      "  model-engine-file   : Absolute path to the pre-generated serialized engine file for the model\n",
      "                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state\n",
      "                        String. Default: \"\"\n",
      "  batch-size          : Maximum batch size for inference\n",
      "                        flags: readable, writable, changeable only in NULL or READY state\n",
      "                        Unsigned Integer. Range: 1 - 1024 Default: 1 \n",
      "  interval            : Specifies number of consecutive batches to be skipped for inference\n",
      "                        flags: readable, writable, changeable only in NULL or READY state\n",
      "                        Unsigned Integer. Range: 0 - 2147483647 Default: 0 \n",
      "  gpu-id              : Set GPU Device ID\n",
      "                        flags: readable, writable, changeable only in NULL or READY state\n",
      "                        Unsigned Integer. Range: 0 - 4294967295 Default: 0 \n",
      "  raw-output-file-write: Write raw inference output to file\n",
      "                        flags: readable, writable, changeable only in NULL or READY state\n",
      "                        Boolean. Default: false\n",
      "  raw-output-generated-callback: Pointer to the raw output generated callback funtion\n",
      "\t\t\t(type: gst_nvinfer_raw_output_generated_callback in 'gstnvdsinfer.h')\n",
      "                        flags: readable, writable, changeable only in NULL or READY state\n",
      "                        Pointer.\n",
      "  raw-output-generated-userdata: Pointer to the userdata to be supplied with raw output generated callback\n",
      "                        flags: readable, writable, changeable only in NULL or READY state\n",
      "                        Pointer.\n",
      "  output-tensor-meta  : Attach inference tensor outputs as buffer metadata\n",
      "                        flags: readable, writable, changeable only in NULL or READY state\n",
      "                        Boolean. Default: false\n",
      "  output-instance-mask: Instance mask expected in network output and attach it to metadata\n",
      "                        flags: readable, writable, changeable only in NULL or READY state\n",
      "                        Boolean. Default: false\n",
      "  input-tensor-meta   : Use preprocessed input tensors attached as metadata instead of preprocessing inside the plugin\n",
      "                        flags: readable, writable, changeable only in NULL or READY state\n",
      "                        Boolean. Default: false\n",
      "\n",
      "Element Signals:\n",
      "  \"model-updated\" :  void user_function (GstElement* object,\n",
      "                                         gint arg0,\n",
      "                                         gchararray arg1,\n",
      "                                         gpointer user_data);\n"
     ]
    }
   ],
   "source": [
    "!gst-inspect-1.0 nvinfer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7f9ef38-d544-4296-8307-a40b68ce592e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "!gst-inspect-1.0 nvinfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b9fac2-a4d3-4306-a185-11fa0fdae1f0",
   "metadata": {},
   "source": [
    "Click ... to show **solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632dd78b",
   "metadata": {},
   "source": [
    "The `nvinfer` plugin does inferencing on input data using NVIDIA TensorRT. It can perform AI inference on (batched) images for classification, object detection, and segmentation tasks based on the trained model we provide. There are several properties that can be set related to the inference engine, including the `model-engine-file` property. We recommend setting properties via a configuration file through the `config-file-path` property. More information about DeepStream plugins can be found in the [DeepStream Plugin Guide](https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html#plugins-development-guide). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab2e343",
   "metadata": {},
   "source": [
    "<a name='s4'></a>\n",
    "## Accessing DeepStream MetaData ##\n",
    "`GstBuffer` is the basic unit of data transfer in GStreamer. As it's passing through the pipeline, metadata received by each component is attached to the buffer. Similarly, the DeepStream SDK attaches the DeepStream metadata object, `NvDsBatchMeta` to it. DeepStream metadata contains inference results from `Gst-nvinfer` and information from other plugins in the pipeline. DeepStream uses an extensible standard structure for metadata, starting with the batch level metadata (`NvDsBatchMeta`) created inside the `Gst-nvstreammux` plugin. Subsidiary metadata structures hold frame, object, classifier, and display data. The metadata format is described in detail in the [SDK MetaData Documentation and API Guide](https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_metadata.html). Having some familiarity with the metadata structure will help us extract the desired information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab080c8b",
   "metadata": {},
   "source": [
    "<a name='s4.1'></a>\n",
    "### Probe ###\n",
    "<p><img src='images/probe.png' width=720></p>\n",
    "\n",
    "We use [probes](https://gstreamer.freedesktop.org/documentation/application-development/advanced/pipeline-manipulation.html#using-probes) to access this metadata. Probing is best envisioned as having access to a pad listener. We can use them to access metadata at various points in the pipeline. Technically, a probe is a [callback function](https://en.wikipedia.org/wiki/Callback_(computer_programming)) that can be attached to a pad. While attached, the probe notifies when there is data passing on a pad. It allows us to easily interact with the data flowing through our pipeline. For more information on `GstPad` and probes, please visit GStreamer’s API Reference on [GstPad](https://gstreamer.freedesktop.org/documentation/gstreamer/gstpad.html?gi-language=c). \n",
    "\n",
    "<p><img src='images/important.png' width=720></p>\n",
    "\n",
    "Since the video AI application will rely heavily on the metadata generated from the deep learning models, the probe callback function might be the most important piece when constructing a DeepStream pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ed7d1",
   "metadata": {},
   "source": [
    "**Well Done!** When you're ready, let's move to the [next notebook](./03_building_a_DeepStream_application.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac77c87",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
